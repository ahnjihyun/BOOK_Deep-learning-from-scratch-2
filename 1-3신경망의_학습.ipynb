{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch01_1-3신경망의 학습.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/vhJMqztFpsqjdVZnAY6v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahnjihyun/BOOK_Deep-learning-from-scratch-2/blob/main/1-3%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98_%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 신경망의 학습\n",
        "\n",
        "##### (p54~) 1.3.4 계산 그래프\n",
        "역전파 메서드 추가"
      ],
      "metadata": {
        "id": "_TC0CiLNorh9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WYyQyzy-oget"
      },
      "outputs": [],
      "source": [
        "class MatMul: # matrix multiply \n",
        "  def __init__(self,W):\n",
        "    self.params = [W] # 학습하는 매개변수를 params에 보관\n",
        "    self.grads = [np.zeros_like(W)] # 거기에 대응시키는 형태로 기울기를 grads에 보관\n",
        "    self.x = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    W, = self.params\n",
        "    out = np.matmul(x,W)\n",
        "    self.x = x\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    W, = self.params\n",
        "    dx = np.matmul(dout, W.T)\n",
        "    dW = np.matmul(self.x.T, dout)\n",
        "    self.grads[0][...] = dW # 기울기값을 설정하는 코드 \n",
        "    # 생략기호(ellipsis, '...') 사용 => 넘파이 배열이 가리키는 메모리 위치를 고정시킨 다음, 그 위치에 원소들을 덮어씀(깊은 복사)\n",
        "    return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.3.5 기울기 도출과 역전파 구현 \n",
        "Sigmoid 계층"
      ],
      "metadata": {
        "id": "HfjX7s-Yo1Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = 1 / (1+np.exp(-x))\n",
        "    self.out = out\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "    return dx"
      ],
      "metadata": {
        "id": "sUVMYoO5oy7u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Affine 계층"
      ],
      "metadata": {
        "id": "U0WWi5s9o29A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Affine:\n",
        "  def __init__(self,W,b):\n",
        "    self.parans = [W,b]\n",
        "    self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "    self.x = None \n",
        "\n",
        "  def forward(self,x):\n",
        "    W,b = self.params\n",
        "    out = np.matmul(x,W) + b\n",
        "    self.x = x\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    W,b = self.params\n",
        "    dx = np.matmul(dout, W.T)\n",
        "    dW = np.matmul(self.x.T, dout)\n",
        "    db = np.sum(dout, axis=0)\n",
        "\n",
        "    self.grads[0][...] = dW\n",
        "    self.grads[1][...] = db\n",
        "    return dx"
      ],
      "metadata": {
        "id": "H3WMCzaVoy5R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### (p60) 1.3.6 가중치 갱신\n",
        "\n",
        "신경망의 학습은 다음 순서로 수행합니다\n",
        "\n",
        "> - **1단계: 미니배치**\n",
        "> \n",
        ">     훈련데이터 중에서 무작위로 다수의 데이터를 골라낸다 (배치=덩어리)\n",
        "> \n",
        "> - **2단계: 기울기 계산**\n",
        "> \n",
        ">     오차역전파법으로 각 가중치 매개변수에 대한 손실 함수의 기울기를 구한다\n",
        "> \n",
        "> - **3단계: 매개변수 갱신**\n",
        "> \n",
        ">     기울기를 사용하여 가중치 매개변수를 갱신한다\n",
        "> \n",
        "> - **4단계: 반복**\n",
        "> \n",
        ">     1~3단계를 필요한 만큼 반복한다"
      ],
      "metadata": {
        "id": "wpjVLUO1o8Wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "오차역전파법으로 가중치의 기울기를 얻는다.\n",
        "\n",
        "이 기울기는 현재의 가중치 매개변수에서 손실을 가장 크게 하는 방향을 가리킨다.\n",
        "\n",
        "따라서 매개변수를 그 기울기와 반대 방향으로 갱신하면 손실을 줄일 수 있다\n",
        "\n",
        "=> 경사하강법(Gradient Discent)\n"
      ],
      "metadata": {
        "id": "SGQRxo0Mo-Pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### (p61) 확률적 경사하강법(Stochastic Gradient Discent) 구현"
      ],
      "metadata": {
        "id": "MgMQ-aGkpAqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "  def __init__(self, lr=0.01): # 초기화 인수 lr은 학습률(learning rate)를 의미\n",
        "    self.lr = lr\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    for i in range(len(params)):\n",
        "      params[i] -= self.lr * grads[i]"
      ],
      "metadata": {
        "id": "mTWOxugXo6P5"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}